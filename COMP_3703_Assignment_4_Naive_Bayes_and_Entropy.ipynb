{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xlzuvekas/Machine-Learning/blob/main/COMP_3703_Assignment_4_Naive_Bayes_and_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Naive Bayes and Entropy\n",
        "\n",
        "Course: COMP 3703 - Natural Language Processing\n",
        "\n",
        "Due: Wednesday, February 8 at 11:59pm Mountain time"
      ],
      "metadata": {
        "id": "3I-2a5gpUEBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "YOUR NAME HERE: Xavier Zuvekas\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jSONDNoFUMQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Naive Bayes\n",
        "\n",
        "To avoid long explanations of how to use the `nltk` library's function calls since there are quite a few related to this problem, the code cells below set up most of the problem for you.\n",
        "\n",
        "First, import everything we need and download the appropriate packages."
      ],
      "metadata": {
        "id": "URy7dYzVU1i8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeFkfMiIT5sJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22dd8de2-330b-4b3a-e159-0e6d0f0f59db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The documents we want are in the `movie_reviews` corpus, but they are not formatted in a way that is nice for training a Naive Bayes classifier. Conveniently, the review text is already tokenized and lowercased, so we don't have to worry about that. The code below reformats the documents for you, with comments explaining what's happening on each line."
      ],
      "metadata": {
        "id": "fmtwHIKy0wwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []                                                    # Our custom dataset list\n",
        "for label in movie_reviews.categories():                     # Categories are 'neg' and 'pos'\n",
        "  files = movie_reviews.fileids(label)                       # Get list of file names associated with label\n",
        "  for file in files:\n",
        "    raw_words_from_file = movie_reviews.words(file)          # Get words in file (one movie review)\n",
        "    word_set_with_features = FreqDist(raw_words_from_file)   # Use word counts as features\n",
        "    words_with_label = (word_set_with_features, label)       # Put features together with label in a tuple\n",
        "    docs.append(words_with_label)                            # Add tuple to our docs list\n",
        "\n",
        "docs[:5]                                                     # Print a few docs to see formatting"
      ],
      "metadata": {
        "id": "o9x6oOpbVjWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbf2806-4c9d-4e4f-c597-0736993e7cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(FreqDist({',': 44, 'the': 38, '.': 34, 'it': 25, 'and': 20, 'to': 16, 'of': 16, \"'\": 16, 'a': 14, 'that': 13, ...}),\n",
              "  'neg'),\n",
              " (FreqDist({',': 18, '.': 14, 'the': 13, \"'\": 13, 'a': 13, 's': 9, 'and': 8, 'of': 8, 'movie': 5, 'that': 4, ...}),\n",
              "  'neg'),\n",
              " (FreqDist({'the': 42, ',': 31, '.': 22, 'is': 17, \"'\": 15, 'that': 13, 'of': 13, 'and': 12, 'it': 10, 'a': 10, ...}),\n",
              "  'neg'),\n",
              " (FreqDist({',': 26, \"'\": 24, 'the': 21, '\"': 20, '.': 20, 's': 18, '-': 15, 'to': 14, 'and': 11, 'of': 11, ...}),\n",
              "  'neg'),\n",
              " (FreqDist({',': 54, 'the': 48, '.': 37, 'to': 26, 'a': 20, 'is': 18, 'and': 17, \"'\": 15, 'of': 15, '-': 15, ...}),\n",
              "  'neg')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimately, you should end up with a variable called `docs` that is a list of tuples. Each tuple has a set of features - represented with the `FreqDist` class from the last assignment - and the associated label for that review, either `neg` or `pos`. Remember that features are any quantifiable characteristic of the text. In this case, we use counts of each token in the review text as features.\n",
        "\n",
        "For example, if the (completely made up) review is\n",
        "```\n",
        "This is a bad movie that is bad because it's bad.\n",
        "```\n",
        "the actual word list in the `movie_reviews` corpus may have pre-tokenized this review to be something like\n",
        "```\n",
        "['this', 'is', 'a', 'bad', 'movie', 'that', 'is', 'bad', 'because', 'it', '\\'', 's', 'bad', '.']\n",
        "```\n",
        "Notice that alphabetical characters are lowercased and punctuation is included. Making a `FreqDist` out of this text would produce\n",
        "```\n",
        "FreqDist({'bad': 3, 'is': 2, 'this': 1, 'a': 1, 'movie': 1, 'that': 1, ...})\n",
        "```\n",
        "The `NaiveBayesClassifier` expects features to be provided in this type of dictionary format, with the keys being the feature and the values being the associated numerical quantity."
      ],
      "metadata": {
        "id": "wBSJEEl62a_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (a)\n",
        "\n",
        "Before we start training and classifying, as with any machine learning dataset, we must partition the data into two parts: **the train set and the test set** (we are not using the dev set here). The exact cutoff for how much should be in each set is not a standardized nor provably definitive value, so let's just go with **90% train, 10% test**.\n",
        "\n",
        "Split the `docs` list into two separate lists:\n",
        "* a train set list with the **first 90%** of the documents\n",
        "* a test set list with the **last 10%** of the documents\n",
        "\n",
        "Print the lengths of each set to show how many documents are in each set.\n",
        "\n",
        "*Hint: remember if you do floating point computation but want an integer, you can cast the result to an int with `int()`.*"
      ],
      "metadata": {
        "id": "QpZJaAM64MC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "train_set = docs[:int(0.9 * len(docs))]\n",
        "test_set = docs[int(0.9 * len(docs)):]\n",
        "\n",
        "print(\"Length of train set:\", len(train_set))\n",
        "print(\"Length of test set:\", len(test_set))"
      ],
      "metadata": {
        "id": "6jK33Z0cW0oc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5921bca1-cb79-4a3d-b97b-5aa8b51fc350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train set: 1800\n",
            "Length of test set: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (b)\n",
        "\n",
        "Now to actually train the classifier. `NaiveBayesClassifier` is already imported for you. The method we will use to get a trained classifier is\n",
        "```\n",
        "NaiveBayesClassifier.train(your_train_set_variable)\n",
        "```\n",
        "Store the result in a variable as your classifier. The resulting model has a few useful functions.\n",
        "\n",
        "Print out the output from the following functions on your classifier:\n",
        "* `your_classifier_variable.labels()` - should show that `neg` and `pos` are the labels in our dataset\n",
        "* `your_classifier_variable.most_informative_features(10)` - shows the features (the tokens in this case) that are most useful for classifying a review as `neg` or `pos`\n",
        "\n",
        "Do the most informative features make sense to you that they would be useful tokens for distinguishing between positive and negative reviews? You may optionally leave a comment below on your observations."
      ],
      "metadata": {
        "id": "s6tMwd-65jq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "print(\"Labels:\", classifier.labels())\n",
        "print(\"Most Informative Features:\", classifier.most_informative_features(10))"
      ],
      "metadata": {
        "id": "49oVNGHLaJsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74b51f2-85c6-4643-b2ee-9bf13cc9dec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: ['neg', 'pos']\n",
            "Most Informative Features: [('sucks', 1), ('wonderful', 2), ('breathtaking', 1), ('avoids', 1), ('stupid', 2), ('outstanding', 1), ('terrific', 2), ('boring', 2), ('bad', 5), ('captures', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (c)\n",
        "\n",
        "Finally, we must test the classifier. We will later learn more formal approaches on how to do this, but for now we will use **accuracy** as our measurement of success. Accuracy is simply calculated as:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Number of reviews in test set classified correctly}}{\\text{Number of reviews in test set}}\n",
        "$$\n",
        "\n",
        "The relevant function call here is `your_classifier_variable.classify(feature_set)`, which will produce one of the two labels `neg` or `pos` based on the model's internal calculations. The precise mathematics of those calculations are discussed in the lecture slides and textbook.\n",
        "\n",
        "Calculate and print the accuracy of your classifier by iterating over the test set, classifying each review using its feature set, counting the number of reviews classified correctly, and dividing by the number of reviews in the test set.\n",
        "\n",
        "*You may be surprised by how high (close to 1.0) the accuracy is. This is addressed in part (d).*"
      ],
      "metadata": {
        "id": "2FZh7cB37QSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "correct = 0\n",
        "for review in test_set:\n",
        "  feature_set, label = review\n",
        "  predicted_label = classifier.classify(feature_set)\n",
        "  if predicted_label == label:\n",
        "    correct += 1\n",
        "\n",
        "accuracy = correct / len(test_set)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Cc8wZ1E-VZcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5906bf-f275-4ac9-f477-477c2b060679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (d)\n",
        "\n",
        "In part (c), you should find that the accuracy is really, really good. Like almost 100%. There are certainly classifiers that can produce this accuracy, but this is unrealistic for a first shot at a Naive Bayes classifier. Here we will explore why.\n",
        "\n",
        "Recall that `docs` is the overall dataset of reviews and feature sets that you partitioned in part (a). The way `docs` was constructed was by appending on all the reviews associated with the label `neg`, then appending on all the reviews associated with the label `pos`. Since there happens to be an equal number of positive and negative reviews, that means the first 50% of the documents had the label `neg` and the last 50% had the label `pos`.\n",
        "\n",
        "You then split the list into the first 90% being train and the last 10% being test. **Why, then, is our accuracy so high? What is it about this partition that made it very easy for your Naive Bayes classifier to classify this specific test set?**\n",
        "\n",
        "**Provide your answer in the text cell below.** To further confirm your answer, try running the following code cell, which shuffles (randomizes the positioning of the elements in) the `docs` variable, then re-run your code in parts (a), (b), and (c) to re-train your classifier on the shuffled documents and see if your accuracy changes. You will likely get an accuracy of around `.65` to `.75` rather than `.97` or above."
      ],
      "metadata": {
        "id": "fkPxa0f68_hn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "The accuracy is high because of the way the docs list was constructed and partitioned into the train and test sets. As mentioned, the first 50% of the documents had the label negative and the last 50% had the label positive. Since the train set was made up of the first 90% of the documents, it consisted of mostly negative reviews, with a slightly lower amount of positives. The test set was constructed of all positive reviews. This meant that when the Naive Bayes classifier was trained on the train set, it became  biased towards classifying negative reviews, which in turn led to a high accuracy when classifying the test set of mostly positive reviews. \n",
        "\n",
        "When we shuffled the docs, a lower accuracy of 0.69 was observed. This was likely an artifact of the classifier's bias towards negative reviews."
      ],
      "metadata": {
        "id": "_80S7Sr3BfFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(docs) # Re-run parts (a), (b), and (c) to see a more realistic accuracy"
      ],
      "metadata": {
        "id": "dKQNkI07BhdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Entropy with Wordle\n",
        "\n",
        "You may have come across a fun little word game called Wordle before. It started off as a personal project by Josh Wardle (you can see where he got the name from) who designed the game for his partner, Palak Shah, who is a fan of word games.\n",
        "\n",
        "Since the game was released online, it grew immediately in popularity, to the point where [it is now hosted on the New York Times](https://www.nytimes.com/games/wordle/index.html) website and is featured in the NYT phone app alongside their famous crossword puzzles.\n",
        "\n",
        "Linked below is a YouTube video by the wonderful channel [3Blue1Brown](https://www.youtube.com/@3blue1brown), which has loads of videos with fascinating mathematics, computer science, and other STEM-related topics explained with brilliant visuals. This video explores Wordle, how it's played, and how a Wordle session is an exercise in information theory and entropy.\n",
        "\n",
        "**Watch the video, then answer the questions below.** Closed captioning is available if you need it.\n",
        "\n",
        "Video link: https://www.youtube.com/watch?v=v68zYyaEmEA \n",
        "\n",
        "*If you enjoyed that video, you might consider (optionally) watching the follow-up video:* https://www.youtube.com/watch?v=fRed0Xmc2Wg&t=378s"
      ],
      "metadata": {
        "id": "wePYLEghCJ3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Explain in your own words: what does it mean for an observation to have 1 bit of information?**"
      ],
      "metadata": {
        "id": "x66dlYwOEGlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Explain in your own words: what is entropy in this context?** The video offers a few different definitions. What definition makes the most sense to you?"
      ],
      "metadata": {
        "id": "qv80kbjjE9_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "1 bit of information is I=-log2(p)\n",
        "\n",
        "It represents a binary classifcation of either true (1) or false (0). The numbers between the two represent the confidence one way or another. For example, 0.85 would be 85% likely to be a 1."
      ],
      "metadata": {
        "id": "6Za-JCPlHb7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "In this context, entropy is used to determine how much a guess can narrow down the possible world pool. By maximising the entropy of each guess, the average potential information gain will be higher than guessing randomly."
      ],
      "metadata": {
        "id": "40u-6UYbHe35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**From a human perspective, how do you think applying the knowledge or the approach described in this video to a game like Wordle effects the gameplay experience?** This is a completely subjective question. You might describe how your style of Wordle gameplay might change or not change knowing what you've learned from this video. You might compare this to other programs that have been designed to play Starcraft or chess or go. You might be ambivalent and compartmentalize your personal enjoyment of a game from the more logical aspects of it. Feel free to answer how you wish."
      ],
      "metadata": {
        "id": "BplKCwj0FX-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "From my perspective, approaching a game such as wordle through the lens of information theory is extremely intriguing, despite it having potential to 'take the fun out of the game.' Games are as fun as you make them, and given that wordle is a single player game there is no harm in 'cheating.' It is simply offering a different, albeit more informed way to play. The key being that it is single player vs multiplayer game. Should an algorithm be used for games like chess or go, it would certainly be changing the gameplay experience in an unintended manner, offering a severe advantage to one player. "
      ],
      "metadata": {
        "id": "_N7q1dOQHg8h"
      }
    }
  ]
}