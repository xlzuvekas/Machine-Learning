{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xlzuvekas/Machine-Learning/blob/main/COMP_3703_Assignment_7_Word_Vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I-2a5gpUEBc"
      },
      "source": [
        "# Assignment 7: Word Vectors\n",
        "Course: COMP 3703 - Natural Language Processing\n",
        "\n",
        "Due: Friday, March 3 at 11:59pm Mountain time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSONDNoFUMQb"
      },
      "source": [
        "---\n",
        "\n",
        "YOUR NAME HERE: Xavier Zuvekas\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URy7dYzVU1i8"
      },
      "source": [
        "# Preface\n",
        "\n",
        "The focus of this assignment will be the term-document matrix and TF-IDF weighted values. Word2Vec was also covered this past week, but for the sake of scaling the assignment length appropriately we'll leave that out for now. Regardless, by the end of the assignment you will be able to compute and compare word vectors as discussed in lecture.\n",
        "\n",
        "As always, to start, let's import and download the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CaHDjjms8nkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1e5142-59c4-473d-88b3-f17fe049afab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "### RUN THIS ###\n",
        "import nltk\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from math import log\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import gutenberg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the corpus we are using here is from [Project Gutenberg](https://www.gutenberg.org/), an online resource of thousands of e-books. Only a handful of texts are included in `nltk`'s corpus. You can see the files names by running the code cell below."
      ],
      "metadata": {
        "id": "G6e2keEclFcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.fileids() # List of available documents in the Gutenberg corpus"
      ],
      "metadata": {
        "id": "Nx6nf-R_G0FW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de00fe66-920f-45ab-8d44-a9a64f24123e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To decrease the amount of text to be processed (your code would run for quite a long time if we used all of the above texts), we will just stick with the documents written by Jane Austen and William Shakespeare, each of which have 3 documents. The total of 6 texts are extracted for you in the following code cell and stored in a dictionary called `docs`.\n",
        "\n",
        "`docs` is structured so that the file names are the keys and the values are the list of lowercased words associated with that file. The code prints the first 30 words from Shakespeare's *Hamlet* by getting the value associated with `\"shakespeare-hamlet.txt\"` and slicing the first 30 strings.\n",
        "\n",
        "`sorted_doc_names` is also present in the next code cell. This is a list of the 6 document names (their file names) in sorted order. `sorted_doc_names` will be useful to reference later since **term-document matrices and TF-IDF matrices must have the documents (the columns) in a specific order**. A straightforward choice is sorted, alphabetical order."
      ],
      "metadata": {
        "id": "TWiMPtZHlZi_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LSKXrQac8rFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6903bc82-dccd-497c-a8fc-4d117210a0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt']\n",
            "['[', 'the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare', '1599', ']', 'actus', 'primus', '.', 'scoena', 'prima', '.', 'enter', 'barnardo', 'and', 'francisco', 'two', 'centinels', '.', 'barnardo', '.', 'who', \"'\", 's', 'there', '?']\n"
          ]
        }
      ],
      "source": [
        "### RUN THIS ###\n",
        "docs = {}\n",
        "for fileid in gutenberg.fileids():\n",
        "  if fileid.startswith(\"shakespeare\") or fileid.startswith(\"austen\"): # Only get Shakespeare and Austen documents\n",
        "    words = gutenberg.words(fileid)\n",
        "    docs[fileid] = [word.lower() for word in words] # Associate the file name with the lowercased words from that file\n",
        "\n",
        "sorted_doc_names = sorted(docs.keys()) # List of the document/file names in alphabetical order\n",
        "print(sorted_doc_names)\n",
        "\n",
        "hamlet_words = docs['shakespeare-hamlet.txt'] # Example: To access the words from Hamlet, use Hamlet file name\n",
        "print(hamlet_words[:30]) # The associated value with that file name key is the list of words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Term-Document Matrix\n",
        "\n",
        "Your first task is the construct a term-document matrix from our corpus, which is a subset of `nltk`'s selection of Gutenberg texts. Remember that a term-document matrix associates each row with a word/token and each column with a document. In this case, there will be 6 columns since we have 6 documents."
      ],
      "metadata": {
        "id": "_qnLO72QnSaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (a)\n",
        "\n",
        "The number of rows in the term-document matrix is determined by the size of the vocabulary. So before we can create the matrix, we have to compute the vocabulary.\n",
        "\n",
        "Whereas a document may have a given word present multiple times, a vocabulary just lists each word once. What data structure would make the most sense to use to represent a vocabulary then? One solid option is a set. Remember that an empty set can be created using Python's built in `set()` function. Sets also have an `add(new_element)` function for adding new elements to the set.\n",
        "\n",
        "Much like how `sorted_doc_names` will be useful for referencing which column corresponds to which document, we will also want to do the same for each row corresponding to the right word. The most straightforward ordering of strings is alphabetical order, plus we need to be able to index into the vocabulary. \n",
        "\n",
        "This leads to the following steps:\n",
        "\n",
        "* **Extract the vocabulary of the corpus (all documents) as a set.** The point of using a set is to avoid duplicates.\n",
        "* **Convert that set to a list and sort the list.** This variable will be useful later to know what word each row corresponds to.\n",
        "* **Print the length of the list to see the size of the vocabulary.**"
      ],
      "metadata": {
        "id": "5m-tVeEhnnpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OlRJsBzg9K4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "411ec3a0-4a5e-4fe4-ae7b-52e3aa934880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 15683\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE ###\n",
        "vocab_set = set()\n",
        "for doc_name in sorted_doc_names:\n",
        "    words = docs[doc_name]\n",
        "    for word in words:\n",
        "        vocab_set.add(word)\n",
        "\n",
        "vocab_list = sorted(list(vocab_set))\n",
        "print(\"Vocabulary size:\", len(vocab_list))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "\n",
        "Now we can create the matrix. You could choose to use a list of lists here, but as you will see later, it is much more convenient to use `numpy`'s matrices and arrays. `numpy` is imported as `np`, as per convention.\n",
        "\n",
        "`np.zeros(shape)` is a useful function for initializing arrays and matrices, namely with 0 for every value. To create a matrix, `shape` must be specified as a tuple with two values. For example, `np.zeros((2,3))` (note the parentheses) would create the matrix\n",
        "```\n",
        "array([[0., 0., 0.],\n",
        "       [0., 0., 0.]])\n",
        "```\n",
        "The first value of the tuple for `shape` corresponds to the number of rows. The second value corresponds to the number of columns.\n",
        "\n",
        "Of course, this matrix will not just contain zeros. The element at row `i`, column `j` of your term-document matrix should have the term frequency of the word corresponding to row `i` in the document corresponding to column `j`.\n",
        "\n",
        "**Create the term-document matrix for our corpus of 6 documents. Print the matrix at the end to at least confirm it at least contains non-zero values.**\n",
        "\n",
        "*This code cell will take the longest to run of everything in this notebook, maybe around 1-3 minutes depending on your connection. In fact, if you've ever used an application (e.g. an IDE like IntelliJ or Pycharm) that shows the word \"Indexing...\" when it's loading, it's because the application is doing something similar to this for your files.*\n",
        "\n",
        "*Suggestion: To create the matrix, you will have to iterate over all the words and all the documents. It may be useful to have the outer loop correspond to the documents so you can print which document you're currently on and know how long it's taking to process.*"
      ],
      "metadata": {
        "id": "I-HwWtS0pgfX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4QZVhuwFAsZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "771c9807-1005-4dda-84ca-962de1d41c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed document 1/6\n",
            "Processed document 2/6\n",
            "Processed document 3/6\n",
            "Processed document 4/6\n",
            "Processed document 5/6\n",
            "Processed document 6/6\n",
            "[[549. 253. 289.  21.  17.   5.]\n",
            " [144.  61. 113.   0.   0.   0.]\n",
            " [ 16.   0.  13.   0.   0.   0.]\n",
            " ...\n",
            " [  0.   1.   1.   0.   0.   0.]\n",
            " [  1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE ###\n",
        "# Step 1: Initialize the numpy matrix\n",
        "term_doc_matrix = np.zeros((len(vocab_list), len(sorted_doc_names)))\n",
        "\n",
        "# Step 2: Iterate over each document\n",
        "for j, doc_name in enumerate(sorted_doc_names):\n",
        "    words = docs[doc_name]\n",
        "    \n",
        "    # Step 3: Iterate over each word in the document\n",
        "    for word in words:\n",
        "        # Step 4: Increment the corresponding element in the numpy matrix\n",
        "        i = vocab_list.index(word)\n",
        "        term_doc_matrix[i, j] += 1\n",
        "    \n",
        "    print(f\"Processed document {j+1}/{len(sorted_doc_names)}\")\n",
        "\n",
        "# Step 5: Print the resulting matrix\n",
        "print(term_doc_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: TF-IDF\n",
        "\n",
        "Term-document matrices are not typically used because raw frequencies tend not to correspond to the actual amount of information for each word. A TF-IDF-weighted matrix, though, is often more accurate in this regard.\n",
        "\n",
        "Remember that the TF-IDF value for any given term $t$ in document $d$ in our corpus of $N$ documents is a combination of term frequency and inverse document frequency:\n",
        "\n",
        "$$\n",
        "\\text{tf}_{t,d} = \\log(count(t,d)+1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{idf}_{t} = \\log(\\frac{N}{df_t})\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_{t}\n",
        "$$"
      ],
      "metadata": {
        "id": "unWhgRI6rVNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (a)\n",
        "\n",
        "Before getting into the code, here is a short-answer response question first. Notice that $\\text{tf}_{t,d}$ uses add-one smoothing to prevent taking the log of 0, which is undefined. However, $\\text{idf}_t$ does not add one (to the denominator nor the ratio) even though division by 0 is undefined.\n",
        "\n",
        "**Why does the formula for $\\textbf{idf}_t$ not require add-one smoothing? Answer in your own words in the following text cell.** *Hint: consider how the term-document matrix is constructed.*"
      ],
      "metadata": {
        "id": "YSf3KzO4smNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "The formula for idft does not require add-one smoothing because the denominator of the idft formula, Ndft (i.e., the number of documents in the corpus containing the term t), can never be zero. This is because the term-document matrix is constructed in such a way that all words in the vocabulary are represented as rows, and each row has a non-zero count in at least one document (even if it's just the document in which the word appears). Therefore, every term in the vocabulary will have a non-zero Ndft value, making add-one smoothing unnecessary for the idft formula."
      ],
      "metadata": {
        "id": "itVc8h07tIqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "\n",
        "**Create another matrix, based on your term-document matrix from the previous question to avoid re-reading all the documents, with TF-IDF weighted values instead of raw frequencies.** This one should not take long like the original matrix did since you're not processing based on the strings in each document.\n",
        "\n",
        "*For the next question it is important that you have both the original term-document matrix and the TF-IDF matrix, so do not overwrite the matrix from above.*"
      ],
      "metadata": {
        "id": "e4KHUvFVtKEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "import numpy as np\n",
        "\n",
        "# load the term-document matrix from the previous question\n",
        "\n",
        "\n",
        "# calculate IDF values\n",
        "N = term_doc_matrix.shape[1]\n",
        "idf = np.log(N / (1 + np.count_nonzero(term_doc_matrix, axis=1)))\n",
        "\n",
        "# calculate TF-IDF values\n",
        "tf_idf_matrix = np.zeros(term_doc_matrix.shape)\n",
        "for i in range(term_doc_matrix.shape[0]):\n",
        "    for j in range(term_doc_matrix.shape[1]):\n",
        "        tf_idf_matrix[i][j] = term_doc_matrix[i][j] * idf[i]\n",
        "\n",
        "# save the TF-IDF matrix for future use\n",
        "np.save('tf_idf_matrix.npy', tf_idf_matrix)\n"
      ],
      "metadata": {
        "id": "FvCZrg6wYIMh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: Calculating Similarity\n",
        "\n",
        "Here is where we put our matrices to use. The `numpy` matrices only contain the numerical values, not the associated words and document names, so to get a word or document vector we have an intermediate step. Parts (a) and (b) will address these by defining some useful functions."
      ],
      "metadata": {
        "id": "6kDSD6jMvZy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (a)\n",
        "\n",
        "**Define a function called `word_vector` that takes in a word/token and a `numpy` matrix, and returns the row from the given matrix associated with the given word.** This will involve getting the word's index in the sorted list of vocabulary words, then indexing into the matrix. Indeed, you will be referencing a variable you created in the first question within this function.\n",
        "\n",
        "**Print the word vector for `\"the\"` using the term-document matrix, then print the word vector for `\"the\"` using the TF-IDF matrix.** Although the results are both plausible word vectors for the same word, the values should be notably different in each and should confirm your intuition on what the results should be for a word like \"the\".\n",
        "\n",
        "*Hint: Python lists have a built-in function called `index` that will give you the index of (the first instance of, when there are duplicates) the given element.*"
      ],
      "metadata": {
        "id": "4-de8r-gwkHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "def word_vector(word, matrix):\n",
        "    index = vocab_list.index(word)\n",
        "    return matrix[index]\n"
      ],
      "metadata": {
        "id": "zbHE5-JwEd10"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "\n",
        "**Define a function called `doc_vector` that takes in a document/file name (as originally formatted with the `.txt` extension) and a `numpy` matrix, and returns the column from the given matrix associated with the given document.** This is where `numpy`'s arrays come in handy. Getting a column from a list of lists in basic Python is tricky. `numpy` makes it easy with the following syntax.\n",
        "\n",
        "To get column `col` from a matrix `matrix`, you can use\n",
        "```\n",
        "matrix[:, col] # Outputs column col\n",
        "```\n",
        "This makes use of `numpy`'s slicing syntax, much like how you can slice a 1D list. The `:` means \"give me every row\" since the absence of a start and end row on either side of the `:` defaults to the start and end being 0 and `n` for `n` total rows.\n",
        "\n",
        "**Print the document vector for `\"shakespeare-hamlet.txt\"` using the term-document matrix, then print the document vector for `\"shakespeare-hamlet.txt\"` using the TF-IDF matrix. Also print the length of either vector to confirm that it matches the size of the vocabulary computed previously.** "
      ],
      "metadata": {
        "id": "yJ-b5G4wxq7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "def doc_vector(doc_name, matrix):\n",
        "    index = sorted_doc_names.index(doc_name)\n",
        "    return matrix[:, index]\n",
        "\n",
        "# Using term-document matrix\n",
        "hamlet_td = doc_vector('shakespeare-hamlet.txt', term_doc_matrix)\n",
        "print(\"Term-document matrix vector for Hamlet:\\n\", hamlet_td)\n",
        "print(\"Length of term-document matrix vector for Hamlet:\", len(hamlet_td))\n",
        "\n",
        "# Using TF-IDF matrix\n",
        "hamlet_tfidf = doc_vector('shakespeare-hamlet.txt', tf_idf_matrix)\n",
        "print(\"\\nTF-IDF matrix vector for Hamlet:\\n\", hamlet_tfidf)\n",
        "print(\"Length of TF-IDF matrix vector for Hamlet:\", len(hamlet_tfidf))\n",
        "\n"
      ],
      "metadata": {
        "id": "Gq1NMUTkLDsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58387a3-9883-4419-b4c4-b5b5dc68d45b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term-document matrix vector for Hamlet:\n",
            " [17.  0.  0. ...  0.  0.  1.]\n",
            "Length of term-document matrix vector for Hamlet: 15683\n",
            "\n",
            "TF-IDF matrix vector for Hamlet:\n",
            " [-2.62056156  0.          0.         ...  0.          0.\n",
            "  1.09861229]\n",
            "Length of TF-IDF matrix vector for Hamlet: 15683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (c)\n",
        "\n",
        "**Define a function called `sim` or `similarity` that takes in two vectors (1-dimensional `numpy` arrays) and returns the cosine similarity between the two vectors.** Remember that cosine similarity is defined as:\n",
        "\n",
        "$$\n",
        "\\text{cos}(\\theta) = \\frac{\\textbf{a}\\cdot\\textbf{b}}{|\\textbf{a}||\\textbf{b}|}\n",
        "$$\n",
        "\n",
        "$\\textbf{a}\\cdot\\textbf{b}$ is the dot product between vectors $\\textbf{a}$ and $\\textbf{b}$. $|\\textbf{a}|$ is the L2 norm of $\\textbf{a}$ which can be easily calculated using `numpy.linalg`'s `norm` function.\n",
        "\n",
        "**Get the word vectors for `\"king\"`, `\"queen\"`, and `\"apple\"` (or any other very different third word that appears in these documents) using your `word_vector` function based on the term-document matrix. Print the similarity between `\"king\"` and `\"queen\"`, then print the similarity between `\"king\"` and `\"apple\"` (or whichever obviously different word you choose) using your `similarity` function.** The results should be pretty intuitive given the word choice.\n",
        "\n",
        "**Do the same thing using the TF-IDF vectors for each word to see how the results differ.** You should find that the TF-IDF-based similarities are more compressed than the term-document ones."
      ],
      "metadata": {
        "id": "B09Yw3L8zYFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "import numpy as np\n",
        "\n",
        "def similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
        "    return dot_product / norm_product\n",
        "\n",
        "# Get word vectors for \"king\", \"queen\", and \"apple\"\n",
        "king_vec = word_vector(\"king\", term_doc_matrix)\n",
        "queen_vec = word_vector(\"queen\", term_doc_matrix)\n",
        "apple_vec = word_vector(\"apple\", term_doc_matrix)\n",
        "\n",
        "# Compute similarities using the term-document matrix\n",
        "print(\"Similarity between king and queen (term-document matrix):\", similarity(king_vec, queen_vec))\n",
        "print(\"Similarity between king and apple (term-document matrix):\", similarity(king_vec, apple_vec))\n",
        "\n",
        "# Get word vectors for \"king\", \"queen\", and \"apple\" using the TF-IDF matrix\n",
        "king_vec_tfidf = word_vector(\"king\", tf_idf_matrix)\n",
        "queen_vec_tfidf = word_vector(\"queen\", tf_idf_matrix)\n",
        "apple_vec_tfidf = word_vector(\"apple\", tf_idf_matrix)\n",
        "\n",
        "# Compute similarities using the TF-IDF matrix\n",
        "print(\"Similarity between king and queen (TF-IDF matrix):\", similarity(king_vec_tfidf, queen_vec_tfidf))\n",
        "print(\"Similarity between king and apple (TF-IDF matrix):\", similarity(king_vec_tfidf, apple_vec_tfidf))\n"
      ],
      "metadata": {
        "id": "c7csBiKGM6ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264f9d81-fd76-46a4-e568-4df8ec0473ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between king and queen (term-document matrix): 0.9319871710504082\n",
            "Similarity between king and apple (term-document matrix): 0.011072051921490599\n",
            "Similarity between king and queen (TF-IDF matrix): 0.9319871710504082\n",
            "Similarity between king and apple (TF-IDF matrix): 0.011072051921490597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (d)\n",
        "\n",
        "This part will attempt to answer the question of *\\\"how similar are Jane Austen's works to William Shakespeare's works?\\\"* using our small corpus of 3 works for each author and the matrices you've constructed above.\n",
        "\n",
        "To do so, you should get the document vectors for each of the documents and compute the **centroid vector** (or just \"centroid\") of the document vectors from Jane Austen and the centroid of the document vectors from Shakespeare. The centroid is the average of a set of vectors. For $k$ vectors:\n",
        "$$\n",
        "\\text{centroid} = \\frac{\\textbf{v}_1+\\textbf{v}_2+...+\\textbf{v}_k}{k}\n",
        "$$\n",
        "where $+$ is element-wise addition between vectors and division by $k$ is element-wise division. Both operations are handled intuitively by `numpy` arrays.\n",
        "\n",
        "**Compute the centroid for Jane Austen's works and the centroid for Shakespeare's works with the matrix of your choice. Compute and print the cosine similarity between the two centroids using your `similarity` function.**\n",
        "\n",
        "*You do not have to do this for both types of vectors for this part (it can be a lot of redundant code). If you do print both, you should find the results are very different.*"
      ],
      "metadata": {
        "id": "FpaFm8OL2nRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_docs(keywords, source, matrix):\n",
        "    \"\"\"\n",
        "    Searches for documents containing all the given keywords using the given matrix.\n",
        "    \n",
        "    Parameters:\n",
        "        keywords (tuple): A tuple of strings representing the keywords to search for.\n",
        "        source (list): A list of strings representing the file names to search through.\n",
        "        matrix (numpy.ndarray): A numpy array representing the term-document or TF-IDF matrix.\n",
        "    \n",
        "    Returns:\n",
        "        A tuple of numpy arrays representing the matches for each keyword.\n",
        "    \"\"\"\n",
        "    # Initialize empty arrays for each keyword\n",
        "    matches = tuple(np.array([]) for _ in keywords)\n",
        "    \n",
        "    # Loop through each file in the source list\n",
        "    for file in source:\n",
        "        # Get the document vector for the file\n",
        "        doc_vec = doc_vector(file, matrix)\n",
        "        \n",
        "        # Check if all keywords are present in the document\n",
        "        if all(keyword in vocab_list for keyword in keywords):\n",
        "            # Loop through each keyword and add the matching entries to the corresponding array\n",
        "            for i, keyword in enumerate(keywords):\n",
        "                keyword_index = vocab_list.index(keyword)\n",
        "                keyword_matches = doc_vec[keyword_index]\n",
        "                matches[i] = np.append(matches[i], keyword_matches)\n",
        "    \n",
        "    return matches\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get the document vectors for Shakespeare and Jane Austen works\n",
        "shakespeare_docs = [doc for doc in docs if 'shakespeare' in doc.lower()]\n",
        "ja_docs = [doc for doc in docs if 'austen' in doc.lower()]\n",
        "\n",
        "shakespeare_term_doc_vectors = [doc_vector(doc, term_doc_matrix) for doc in shakespeare_docs]\n",
        "ja_term_doc_vectors = [doc_vector(doc, term_doc_matrix) for doc in ja_docs]\n",
        "\n",
        "shakespeare_tf_idf_vectors = [doc_vector(doc, tf_idf_matrix) for doc in shakespeare_docs]\n",
        "ja_tf_idf_vectors = [doc_vector(doc, tf_idf_matrix) for doc in ja_docs]\n",
        "\n",
        "\n",
        "# Compute the centroid vector for Shakespeare works and Jane Austen works\n",
        "shakespeare_term_doc_centroid = np.mean(shakespeare_term_doc_vectors, axis=0)\n",
        "ja_term_doc_centroid = np.mean(ja_term_doc_vectors, axis=0)\n",
        "\n",
        "shakespeare_tf_idf_centroid = np.mean(shakespeare_tf_idf_vectors, axis=0)\n",
        "ja_tf_idf_centroid = np.mean(ja_tf_idf_vectors, axis=0)\n",
        "\n",
        "# Compute the cosine similarity between the centroid vectors using the term-document vectors\n",
        "similarity_term_doc = similarity(shakespeare_term_doc_centroid, ja_term_doc_centroid)\n",
        "\n",
        "# Compute the cosine similarity between the centroid vectors using the TF-IDF vectors\n",
        "similarity_tf_idf = similarity(shakespeare_tf_idf_centroid, ja_tf_idf_centroid)\n",
        "\n",
        "# Print the cosine similarities\n",
        "print(f\"Cosine similarity using term-document vectors: {similarity_term_doc}\")\n",
        "print(f\"Cosine similarity using TF-IDF vectors: {similarity_tf_idf}\")\n"
      ],
      "metadata": {
        "id": "y4_f2vqmTGlA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98ac167-93bb-436f-e5d7-7cfef684e57d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity using term-document vectors: 0.8885801784399422\n",
            "Cosine similarity using TF-IDF vectors: 0.7587857384811342\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}