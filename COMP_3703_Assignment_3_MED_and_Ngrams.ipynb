{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xlzuvekas/Machine-Learning/blob/main/COMP_3703_Assignment_3_MED_and_Ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3: Minimum Edit Distance and N-Grams\n",
        "\n",
        "Course: COMP 3703 - Natural Language Processing\n",
        "\n",
        "Due: Wednesday, January 25 at 7pm Mountain time"
      ],
      "metadata": {
        "id": "eE_pbcMvb8Rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "YOUR NAME HERE: Xavier Zuvekas\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "32D_nBMYcCI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Alignment\n",
        "\n",
        "```\n",
        "incendio\n",
        "glacius\n",
        "lumos\n",
        "stupefy\n",
        "accio\n",
        "inflatus\n",
        "scourgify\n",
        "sonorus\n",
        "```\n",
        "\n",
        "For this question you will construct an alignment between two words - a source word and a target word - in the same fashion as the lecture. Choose two words from the above list (which are all spells from Harry Potter) as your source and target words. You may choose any two you like so long as you can align the two words such that **at least two letters remain unchanged between the two words**. For example, the alignment from lecture of `happy` to `happen` has 4 letters in common which remain unchanged, `happ`, whereas `happy` and `lettuce` have no letters in common.\n",
        "\n",
        "Calculate an alignment by eye between your two words and type out the alignment below, replacing the placeholder `source` and `target` with your chosen words. Follow the same syntax as the lecture slides and textbook:\n",
        "\n",
        "* Use a column with a star `*` where necessary to indicate insertion/deletion. The vertical bar `|` key is typed via `Shift+\\`.\n",
        "* Include a `d`, `i`, or `s` below each column to indicate the corresponding operation. Letters that remain unchanged should not be labeled.\n",
        "\n",
        "Your alignment does **not** have to be representative of the minimum edit distance between the two words.\n",
        "\n",
        "Side note: for those new to using Markdown, any text bookended by triple quotes `'''` will be written in the same manner that code would be, but within a text cell instead of a code cell."
      ],
      "metadata": {
        "id": "5ZpjYcp8cNIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE (or in a new cell if you prefer):\n",
        "\n",
        "```\n",
        "incendio -> sonorus\n",
        "i|n|c|e|n|d|i|o\n",
        "d| | |d| | |d|\n",
        "s|o|n|o|r|u|s\n",
        " | | | |i|i| |\n",
        "\n",
        "3 deletions, 2 insertions\n",
        "\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "txEVtBjsEsOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Minimum Edit Distance Algorithm\n",
        "\n",
        "For this question you will implement the minimum edit distance algorithm. Part (a) will set up the three operation functions and part (b) will implement the actual algorithm. Part (c) adds some extra functionality."
      ],
      "metadata": {
        "id": "E649fxjUS9dP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (a)\n",
        "To get warmed up, let's start by implementing the three auxiliary functions, `del_cost`, `ins_cost`, and `sub_cost` as aptly named in the pseudocode. Each of these takes in a character (`sub_cost` takes in two) and returns an integer value. Though the definitions for these functions could be customized to more intricate implementations, we use the *Levenshtein distance* for this question. This means that although `del_cost` and `ins_cost` take in a character as input, the input does not effect what is returned, as detailed below. \n",
        "\n",
        "Define the three functions:\n",
        "* `del_cost`: returns the value 1\n",
        "* `ins_cost`: returns the value 1\n",
        "* `sub_cost`: return 2 if the two input characters do not match, 0 otherwise\n",
        "\n",
        "For now, you only need to define these functions. We will call them in part (b)."
      ],
      "metadata": {
        "id": "gXXArqDwhhu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def del_cost(letter):\n",
        "  ### YOUR CODE HERE ###\n",
        "\n",
        "  return 1  \n",
        "def ins_cost(letter):\n",
        "  ### YOUR CODE HERE ###\n",
        "\n",
        "  return 1\n",
        "def sub_cost(letter_a, letter_b):\n",
        "  ### YOUR CODE HERE ###\n",
        "  if letter_a != letter_b:\n",
        "    return 2\n",
        "  \n",
        "  return 0\n",
        "def print_table(table):\n",
        "  for r in table: \n",
        "    print(str(r))\n",
        "  return table"
      ],
      "metadata": {
        "id": "im4wlZpWOP4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "Here is where you will implement the minimum edit distance algorithm. Refer to the lecture slides or textbook for the pseudocode. Remember that the pseudocode syntax is meant to be universal; you have to translate each line into Python code, namely adjusting punctuation and indexing (you will have to account for off-by-one errors due to how indexes are interpreted).\n",
        "\n",
        "Define the function `min_edit_dist` such that the minimum edit distance between the given source and target strings is calculated as described in lecture / in the pseudocode.\n",
        "\n",
        "\n",
        "Some things you may find useful:\n",
        "* An intuitive way of creating lists is to start with an empty list `[]` and append onto it the elements you want.\n",
        "* Much like how you can \"multiply\" strings (e.g. `'a' * 3` gives you `'aaa'`), you can also multiple lists (e.g. `[1,2] * 3` gives you `[1,2,1,2,1,2]`).\n",
        "* For more seasoned Python programmers: If you'd like practice using another useful Python library, `numpy` provides NumPy arrays which act like Java arrays and can be created fairly easily, though you do have the overhead of looking at [the documentation](https://numpy.org/doc/stable/user/absolute_beginners.html) and learning how to do so if you're not already familiar.\n"
      ],
      "metadata": {
        "id": "09YCCX3bli59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def min_edit_dist(source, target):\n",
        "    m, n = len(target), len(source)\n",
        "    D = [[0 for _ in range(m+1)] for _ in range(n+1)]\n",
        "    for i in range(n+1):\n",
        "      \n",
        "      for j in range(m+1):\n",
        "\n",
        "        if i == 0:\n",
        "                D[i][j] = j\n",
        "        elif j == 0:\n",
        "                D[i][j] = i\n",
        "\n",
        "        elif source[i-1] == target[j-1]:\n",
        "                D[i][j] = D[i-1][j-1]\n",
        "        else:\n",
        "                D[i][j] = 1+ min(D[i][j-1], D[i-1][j], D[i-1][j-1]+1)\n",
        "    print_table(D)\n",
        "    return D[n][m]\n",
        "\n",
        "print(min_edit_dist(\"actual\", \"natural\")) # should be 3  "
      ],
      "metadata": {
        "id": "JrpX8lL0OdfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3887b26c-a42a-4adb-9de6-a099fc94f0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7]\n",
            "[1, 2, 1, 2, 3, 4, 5, 6]\n",
            "[2, 3, 2, 3, 4, 5, 6, 7]\n",
            "[3, 4, 3, 2, 3, 4, 5, 6]\n",
            "[4, 5, 4, 3, 2, 3, 4, 5]\n",
            "[5, 6, 5, 4, 3, 4, 3, 4]\n",
            "[6, 7, 6, 5, 4, 5, 4, 3]\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test your function by calling it on some of the examples from lecture / the textbook here:"
      ],
      "metadata": {
        "id": "Dn276IazpHlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(min_edit_dist(\"happy\", \"happen\")) # should be 3\n",
        "print(min_edit_dist(\"actual\", \"natural\")) # should be 3\n",
        "print(min_edit_dist(\"intention\", \"execution\")) # should be 8"
      ],
      "metadata": {
        "id": "DfDUhkmDPaKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d32feaad-559d-4205-de88-eacc87a109eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "[1, 0, 1, 2, 3, 4, 5]\n",
            "[2, 1, 0, 1, 2, 3, 4]\n",
            "[3, 2, 1, 0, 1, 2, 3]\n",
            "[4, 3, 2, 1, 0, 1, 2]\n",
            "[5, 4, 3, 2, 1, 2, 3]\n",
            "3\n",
            "[0, 1, 2, 3, 4, 5, 6, 7]\n",
            "[1, 2, 1, 2, 3, 4, 5, 6]\n",
            "[2, 3, 2, 3, 4, 5, 6, 7]\n",
            "[3, 4, 3, 2, 3, 4, 5, 6]\n",
            "[4, 5, 4, 3, 2, 3, 4, 5]\n",
            "[5, 6, 5, 4, 3, 4, 3, 4]\n",
            "[6, 7, 6, 5, 4, 5, 4, 3]\n",
            "3\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[1, 2, 3, 4, 5, 6, 7, 6, 7, 8]\n",
            "[2, 3, 4, 5, 6, 7, 8, 7, 8, 7]\n",
            "[3, 4, 5, 6, 7, 8, 7, 8, 9, 8]\n",
            "[4, 3, 4, 5, 6, 7, 8, 9, 10, 9]\n",
            "[5, 4, 5, 6, 7, 8, 9, 10, 11, 10]\n",
            "[6, 5, 6, 7, 8, 9, 8, 9, 10, 11]\n",
            "[7, 6, 7, 8, 9, 10, 9, 8, 9, 10]\n",
            "[8, 7, 8, 9, 10, 11, 10, 9, 8, 9]\n",
            "[9, 8, 9, 10, 11, 12, 11, 10, 9, 8]\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (c)\n",
        "Augment your `min_edit_dist` function above to not only return the minimum edit distance itself, but also a string representing the table of values. **`min_edit_dist` should therefore return a tuple with the distance as the first value and the table string as the second.**\n",
        "\n",
        "To keep formatting simple, you may assume that each value is only 1 digit.\n",
        "\n",
        "Here is an example output for printing the table corresponding to the strings `\"actual\"` and `\"natural\"`:\n",
        "```\n",
        "  * n a t u r a l \n",
        "* 0 1 2 3 4 5 6 7\n",
        "a 1 2 1 2 3 4 5 6\n",
        "c 2 3 2 3 4 5 6 7\n",
        "t 3 4 3 2 3 4 5 6\n",
        "u 4 5 4 3 2 3 4 5\n",
        "a 5 6 5 4 3 4 3 4\n",
        "l 6 7 6 5 4 5 4 3\n",
        "```\n",
        "\n",
        "You may either \n",
        "* add onto the function in the code cell for part (b) directly or\n",
        "* create a separate `min_edit_dist` function definition in a new code cell below and leave part (b)'s implementation untouched. \n",
        "\n",
        "Run the updated function calls to `min_edit_dist` below to test your code."
      ],
      "metadata": {
        "id": "9bgOURcUvjMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE OR IN ABOVE FUNCTION DEFINITION ###\n",
        "def min_edit_dist(source, target):\n",
        "    m, n = len(target), len(source)\n",
        "    D = [[0 for _ in range(m+1)] for _ in range(n+1)]\n",
        "    for i in range(n+1):\n",
        "      \n",
        "      for j in range(m+1):\n",
        "\n",
        "        if i == 0:\n",
        "                D[i][j] = j\n",
        "        elif j == 0:\n",
        "                D[i][j] = i\n",
        "\n",
        "        elif source[i-1] == target[j-1]:\n",
        "                D[i][j] = D[i-1][j-1]\n",
        "        else:\n",
        "                D[i][j] = 1+ min(D[i][j-1], D[i-1][j], D[i-1][j-1]+1)\n",
        "    \n",
        "    table_str = \"  * \" + \" \".join([\"{}\".format(c) for c in target]) + \"\\n\"\n",
        "    for i in range(len(source) + 1):\n",
        "        if i == 0:\n",
        "            table_str += \"* \"\n",
        "        else:\n",
        "            table_str += \"{} \".format(source[i - 1])\n",
        "        for j in range(len(target) + 1):\n",
        "            table_str += \"{} \".format(D[i][j])\n",
        "        table_str += \"\\n\"\n",
        "    \n",
        "    # min_edit_dist should therefore return a tuple with the distance\n",
        "    # as the first value and the table string as the second.\n",
        "    return (D[n][m], table_str)\n"
      ],
      "metadata": {
        "id": "IjbZkJCExDpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(min_edit_dist(\"happy\", \"happen\")[1])     # Assuming table string is the second element returned\n",
        "print(min_edit_dist(\"actual\", \"natural\")[1])"
      ],
      "metadata": {
        "id": "gbcdPjkd8t9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1adf47a-88a6-41b2-a6cc-e3eddea1137a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  * h a p p e n\n",
            "* 0 1 2 3 4 5 6 \n",
            "h 1 0 1 2 3 4 5 \n",
            "a 2 1 0 1 2 3 4 \n",
            "p 3 2 1 0 1 2 3 \n",
            "p 4 3 2 1 0 1 2 \n",
            "y 5 4 3 2 1 2 3 \n",
            "\n",
            "  * n a t u r a l\n",
            "* 0 1 2 3 4 5 6 7 \n",
            "a 1 2 1 2 3 4 5 6 \n",
            "c 2 3 2 3 4 5 6 7 \n",
            "t 3 4 3 2 3 4 5 6 \n",
            "u 4 5 4 3 2 3 4 5 \n",
            "a 5 6 5 4 3 4 3 4 \n",
            "l 6 7 6 5 4 5 4 3 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: N-Gram Models\n",
        "\n",
        "We will use the Brown corpus again to explore a n-gram models using some `nltk` tools. First import `nltk` and download the Brown corpus."
      ],
      "metadata": {
        "id": "OcJSppDpxO93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "j3KXdRRxRvkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5671f909-f533-4bf5-e7c0-3ac41c50ad9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (a)\n",
        "Let's first get a sense of the most common unigrams in the Brown corpus. You can certainly do this using basic Python structures, but let's use `nltk`'s `FreqDist` class. `FreqDist` or \"frequency distribution\" is a data structure very similar to `defaultdict` for those who are familiar. You can use it just like a Python dictionary, but it has some useful extra functions and features. \n",
        "\n",
        "Most conveniently, `FreqDist` has a default value of 0 for each key. So, you do not have to check if a key is present in the `FreqDist` before setting its value. You can just assume any key already exists and has an initial value of 0. This makes counting easier and your code more succinct.\n",
        "\n",
        "Creating an empty `FreqDist` looks something like this:\n",
        "```\n",
        "my_freq_dist = FreqDist()\n",
        "```\n",
        "From there you can increment counts like so (again, without need to check if the key is present already):\n",
        "```\n",
        "my_freq_dist[\"chillax\"] += 1\n",
        "```\n",
        "Of course, we're not interested in the word \"chillax\". Your task is to \n",
        "* construct a frequency distribution of all the unigrams in the Brown corpus, \n",
        "* lowercase all the unigrams before adding them to your distribution to make this case insensitive, and \n",
        "* use the `most_common(n)` function that comes built into your `FreqDist` object to print out the top 10 most common unigrams."
      ],
      "metadata": {
        "id": "vzR4HnSyOcn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "brown_corpus = nltk.corpus.brown.words()\n",
        "\n",
        "unigram_freq_dist = FreqDist()\n",
        "\n",
        "for word in brown_corpus:\n",
        "  unigram_freq_dist[word.lower()] += 1\n",
        "\n",
        "top_10_unigrams = unigram_freq_dist.most_common(10)\n",
        "\n",
        "for u, c in top_10_unigrams:\n",
        "  print(\"{}: {}\".format(u,c))\n"
      ],
      "metadata": {
        "id": "vsz152ZyOjC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8348a69a-df09-4fbe-c6dd-c0f03db36fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the: 69971\n",
            ",: 58334\n",
            ".: 49346\n",
            "of: 36412\n",
            "and: 28853\n",
            "to: 26158\n",
            "a: 23195\n",
            "in: 21337\n",
            "that: 10594\n",
            "is: 10109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "Now let's do the same but for bigrams. `nltk` has a handy function for getting bigrams for you, located in the `nltk.util` package. Given a list of words, `list(bigrams(words))` will return a list of bigrams, namely a list of tuples, produced by those words. You do have to convert the result to a list to use it this way.\n",
        "\n",
        "Let's also pad each sentence with `<s>` and `</s>` to indicate the start and end of a sentence. `nltk.lm.preprocessing` has a function called `pad_both_ends` that will do this for you.\n",
        "\n",
        "```\n",
        "pad_both_ends(['Everything', 'is', 'awesome'], 2)  # 2 is the ngram we're using\n",
        "```\n",
        "will produce\n",
        "```\n",
        "['<s>', 'Everything', 'is', 'awesome', '</s>']\n",
        "```\n",
        "though not as a raw list. This is meant to be used in combination with `bigrams()`, as hinted at by the `2` in the parameter. So the whole pattern you will want to use is\n",
        "```\n",
        "list(bigrams(pad_both_ends(sentence, 2)))\n",
        "```\n",
        "to bigram-ify a given sentence.\n",
        "\n",
        "Your task is to:\n",
        "* create a frequency distribution over the bigrams of the Brown corpus, \n",
        "* with each sentence padded with the special start-of-sentence and end-of-sentence tokens,\n",
        "* all words lowercased to ignore case sensitivity, then\n",
        "* print the 10 most common bigrams.\n",
        "\n",
        "*As a hint for checking your answer, what would naturally be the most common bigram? It involves one of the special sentence tokens.*"
      ],
      "metadata": {
        "id": "JizDkV5lQeMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "bigram_freq_dist = FreqDist()\n",
        "\n",
        "for sentence in brown.sents():\n",
        "  sentence = [word.lower() for word in sentence]\n",
        "\n",
        "  sentence = pad_both_ends(sentence, 2)\n",
        "\n",
        "  bigram_list = list(bigrams(sentence))\n",
        "\n",
        "  for bigram in bigram_list:\n",
        "    bigram_freq_dist[bigram] += 1\n",
        "\n",
        "\n",
        "print(bigram_freq_dist.most_common(10))\n"
      ],
      "metadata": {
        "id": "f6glQKcYpLuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ab16df-5a86-4b98-8ceb-abf4e60c2be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('.', '</s>'), 49346), (('of', 'the'), 9717), (('<s>', 'the'), 6857), ((',', 'and'), 6300), (('in', 'the'), 6025), (('<s>', '``'), 4168), ((',', 'the'), 3783), (('to', 'the'), 3484), ((\"''\", '.'), 3331), (('<s>', 'he'), 2983)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (c)\n",
        "Finally, let's effectively compute conditional probabilities. We'll stick with raw frequency counts instead of probabilities, but this gets across the same idea. \n",
        "\n",
        "Using the techniques we've already established in this question, print out the most likely words to follow the word `the` in the Brown corpus. Recall that this involves \"shrinking your universe\" to only consider bigrams that start with `the`. Within that universe, we want the most likely second words in those bigrams.\n",
        "\n",
        "Write the appropriate code to print the 10 most likely words to follow the word `the` in the Brown corpus."
      ],
      "metadata": {
        "id": "pNJ5w4KiU0Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "from nltk.util import bigrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "bigram_freq_dist = FreqDist()\n",
        "second_word_freq_dist = FreqDist()\n",
        "\n",
        "for sentence in brown.sents():\n",
        "  sentence = [ word.lower() for word in sentence]\n",
        "  sentence = pad_both_ends(sentence,2)\n",
        "  bigrams_list = list(bigrams(sentence))\n",
        "\n",
        "  for bigram in bigrams_list: \n",
        "    bigram_freq_dist[bigram] += 1\n",
        "\n",
        "    if bigram[0] == 'the':\n",
        "      second_word_freq_dist[bigram[1]] += 1\n",
        "       \n",
        "  print(second_word_freq_dist.most_common(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "gqZyppx7NCZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5bbc40e-ed4b-4e9d-a34c-d434d9bccfa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('first', 662)\n",
            "('same', 628)\n",
            "('most', 417)\n",
            "('other', 416)\n",
            "('``', 405)\n",
            "('new', 397)\n",
            "('united', 393)\n",
            "('world', 361)\n",
            "('state', 271)\n",
            "('two', 268)\n"
          ]
        }
      ]
    }
  ]
}