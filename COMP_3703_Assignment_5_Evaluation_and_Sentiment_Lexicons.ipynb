{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xlzuvekas/Machine-Learning/blob/main/COMP_3703_Assignment_5_Evaluation_and_Sentiment_Lexicons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 5: Evaluation and Sentiment Lexicons\n",
        "\n",
        "Course: COMP 3703 - Natural Language Processing\n",
        "\n",
        "Due: Wednesday, February 15 at 11:59pm Mountain time"
      ],
      "metadata": {
        "id": "3I-2a5gpUEBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "YOUR NAME HERE: Xavier Zuvekas\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jSONDNoFUMQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Sentiment Lexicons\n",
        "\n",
        "This assignment very much builds off of the previous assignment. You will effectively create the same model as before using word counts in a Naive Bayes classifier. However, the steps of that process will be a little more formalized here since you will create a second model to compare to the first.\n",
        "\n",
        "To start, import everything we will need by running the code cell below."
      ],
      "metadata": {
        "id": "URy7dYzVU1i8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeFkfMiIT5sJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b18547-216d-4298-daa6-f26e4ab2c1d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "### RUN THIS ###\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.sentiment import vader\n",
        "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the previous assignment, the `movie_reviews` corpus is what we will use and some formatting is done for you here. This formatting code is not quite the same code that was provided last time, however. The documents are gathered for you in a variable called `raw_docs` have the words from each document as a list of strings (unigrams) along with the review label (`'pos'` or `'neg'`).\n",
        "\n",
        "Note that `raw_docs` is shuffled for you here.\n",
        "\n",
        "Run the following code cell to make the `raw_docs` variable and print the first few tuples so you can see the structure of the dataset."
      ],
      "metadata": {
        "id": "YKuOfc5e0JAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "word_counts = []\n",
        "for doc in raw_docs:\n",
        "    tokens = doc[0]\n",
        "    fdist = FreqDist(tokens)\n",
        "    label = doc[1]\n",
        "    word_counts.append((fdist, label))\n",
        "out = word_counts[:5]\n",
        "print(out)"
      ],
      "metadata": {
        "id": "7BZ822KAx66N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25275ca5-3290-4934-db84-85d690d888d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(FreqDist({',': 38, 'the': 32, '.': 29, 'of': 26, 'a': 22, '-': 20, \"'\": 17, 'it': 17, 's': 15, 'that': 10, ...}), 'pos'), (FreqDist({',': 86, 'the': 71, 'of': 48, 'a': 41, 'and': 41, '.': 38, 'that': 33, \"'\": 29, 'to': 26, '-': 23, ...}), 'pos'), (FreqDist({',': 29, '.': 23, \"'\": 20, 'the': 20, 's': 13, 'to': 12, 'it': 11, 'and': 9, 'of': 9, 'a': 8, ...}), 'pos'), (FreqDist({',': 34, '.': 27, 'the': 26, 'a': 16, \"'\": 13, 'of': 11, 'and': 10, 'to': 10, 's': 10, 'it': 10, ...}), 'neg'), (FreqDist({'the': 83, ',': 82, '.': 51, \"'\": 41, 'trek': 36, 'a': 33, 'and': 32, 'of': 31, 'star': 27, ':': 25, ...}), 'pos')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "\n",
        "Here is where the sentiment lexicon comes into play. The lexicon we will use is from the VADER sentiment analyzer in the `nltk.sentiment` package. Run the following code cell to create the `lexicon` variable, which is a Python dictionary with a pre-defined set of words as keys with an associated score reflecting the positivity/negativity of the word as the values."
      ],
      "metadata": {
        "id": "mY4YogMhDrzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### RUN THIS ###\n",
        "lexicon = vader.SentimentIntensityAnalyzer().make_lex_dict() # Get lexicon\n",
        "\n",
        "print(\"happy:\", lexicon['happy'])        # Very positive\n",
        "print(\"terrible:\", lexicon['terrible'])  # Very negative\n",
        "print(\"okay:\", lexicon['okay'])          # Fairly neutral"
      ],
      "metadata": {
        "id": "-9Dk5XjoECDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31aaf890-3ac5-44a1-c553-add06dcd8025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy: 2.7\n",
            "terrible: -2.1\n",
            "okay: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentiment values in `lexicon` will be used as the features of another model, entirely separate from the one based on word counts. Following the same general structure in part (a), create *another new* list based on `raw_docs`. For each tuple in the list,\n",
        "* the first element is a Python dictionary containing:\n",
        "  * keys: all words in the document that appear in `lexicon`\n",
        "  * values: the score from `lexicon` associated with each key\n",
        "* the second element is the label associated with the document, as before.\n",
        "\n",
        "*Recall that you can check if a key `key` is present in a dictionary with a conditional such as* `if key in dict`.\n",
        "\n",
        "Note that **word counts are not accounted for with this feature set**. For example, the review\n",
        "```\n",
        "I am so happy I saw this great movie.\n",
        "```\n",
        "has the same feature set as\n",
        "```\n",
        "I am so happy happy happy happy happy I saw this great movie.\n",
        "```\n",
        "The feature set for *both* reviews would be\n",
        "```\n",
        "{'happy': 2.7,\n",
        " 'great': 3.1}\n",
        "```\n",
        "since `'happy'` and `'great'` are the only tokens in these reviews that appear in `lexicon`. The number of times `'happy'` appears does not change the score.\n",
        "\n",
        "Print out the first few elements of your new sentiment feature-extracted list to confirm the formatting."
      ],
      "metadata": {
        "id": "02-UkVZoFDkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "sentiment_docs = []\n",
        "for doc, label in raw_docs:\n",
        "    sentiment_dict = {}\n",
        "    for word in set(doc):\n",
        "        if word in lexicon:\n",
        "            sentiment_dict[word] = lexicon[word]\n",
        "    sentiment_docs.append((sentiment_dict, label))\n",
        "print(sentiment_docs[:5])\n"
      ],
      "metadata": {
        "id": "jpe8XiMfzsY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c9396c-03fc-431b-b66a-7b6238cbc74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[({'mock': -1.8, 'horror': -2.7, 'blockbuster': 2.9, 'outrage': -2.3, 'caring': 2.2, 'anti': -1.3, 'propaganda': -1.0, 'great': 3.1, 'silly': 0.1, 'dizzy': -0.9, 'fresh': 1.3, 'save': 2.2, 'trite': -0.8, 'killing': -3.4, 'punishing': -2.6, 'devotees': 0.5, 'romantic': 1.7, 'thrilling': 2.1, 'indoctrinated': -0.4, 'rich': 2.6, 'talent': 1.8, 'better': 1.9, 'amusement': 1.5, 'entertainment': 1.8, 'battle': -1.6, 'energy': 1.1, 'strengths': 1.7, 'controversial': -0.8, 'brilliant': 2.8, 'super': 2.9, 'lack': -1.3, 'war': -2.9, 'no': -1.2, 'create': 1.1, 'conflicts': -1.6, 'novel': 1.3, 'worth': 0.9, 'enthusiastically': 2.6, 'hypocritical': -2.0, 'pleasures': 1.9, 'intelligence': 2.1, 'faithfully': 1.8, 'violent': -2.9, 'fun': 2.3, 'hilarious': 1.7, 'like': 1.5, 'battles': -1.6, 'true': 1.8, 'joys': 2.2, 'wish': 1.7, 'plays': 1.0, 'fight': -1.6, 'special': 1.7, 'nonsense': -1.7, 'threatened': -2.0, 'played': 1.4}, 'pos'), ({'debt': -1.5, 'unaware': -0.8, 'inspirational': 2.3, 'ironic': -0.5, 'tears': -0.9, 'beaten': -1.8, 'dream': 1.0, 'eerie': -1.5, 'argument': -1.5, 'anxieties': -0.6, 'surprise': 1.1, 'humor': 1.1, 'shocker': -0.6, 'injury': -1.8, 'landmark': 0.3, 'respected': 2.1, 'dilemma': -0.7, 'emotional': 0.6, 'grim': -2.7, 'prizes': 2.0, 'forget': -0.9, 'death': -2.9, 'doom': -1.7, 'abandoned': -2.0, 'credits': 1.5, 'perfectly': 3.2, 'sure': 1.3, 'friendly': 2.2, 'accident': -2.1, 'shame': -2.1, 'wrong': -2.1, 'bad': -2.5, 'loses': -1.3, 'forgotten': -0.9, 'entertainment': 1.8, 'attractive': 1.9, 'arguably': -1.0, 'discards': -1.0, 'sadness': -1.9, 'stops': -0.6, 'lucky': 1.8, 'embarrassed': -1.5, 'demand': -0.5, 'ambitious': 2.1, 'resolve': 1.6, 'giddy': -0.6, 'love': 3.2, 'beloved': 2.3, 'awesome': 3.1, 'happiness': 2.6, 'shared': 1.4, 'avid': 1.2, 'neglected': -2.4, 'stolen': -2.2, 'suffers': -2.1, 'fears': -1.8, 'tragedies': -1.9, 'profiteers': 0.5, 'perfect': 2.7, 'creations': 1.6, 'fun': 2.3, 'joy': 2.8, 'like': 1.5, 'complacent': -0.3, 'heartbreaking': -2.0, 'hopes': 1.8, 'excite': 2.1, 'improves': 1.8, 'glory': 2.5, 'granted': 1.0, 'challenged': -0.4, 'shares': 1.2, 'joys': 2.2, 'hard': -0.4, 'broken': -2.1, 'safe': 1.9, 'favorite': 2.0, 'rescue': 2.3, 'well': 1.1, 'god': 1.1, 'worst': -3.1, 'energized': 2.3}, 'pos'), ({'pleasantly': 2.1, 'save': 2.2, 'definitely': 1.7, 'ok': 1.2, 'hesitating': -1.4, 'enjoyable': 1.9, 'interested': 1.7, 'heroic': 2.6, 'honest': 2.3, 'surprised': 0.9, 'ruined': -2.1, 'plays': 1.0, 'lack': -1.3, 'welcome': 2.0, 'ruin': -2.8, 'want': 0.3, 'surprise': 1.1, 'succeeds': 2.2, 'unfortunately': -1.4, 'good': 1.9, 'well': 1.1, 'surprises': 0.9, 'infatuated': 0.2, 'promising': 1.7, 'bad': -2.5, 'warns': -0.4, 'determined': 1.4, 'fury': -2.7, 'pretty': 2.2, 'waste': -1.8, 'disaster': -3.1, 'substantial': 0.8}, 'pos'), ({'ghost': -1.3, 'humorous': 1.6, 'like': 1.5, 'thriller': 0.4, 'suspect': -1.2, 'successful': 2.8, 'ridiculous': -1.5, 'surprisingly': 1.2, 'violence': -3.1, 'horror': -2.7, 'entertaining': 1.9, 'award': 2.5, 'winning': 2.4, 'hysterical': -0.1, 'plays': 1.0, 'crappy': -2.6, 'better': 1.9, 'no': -1.2, 'played': 1.4, 'lamely': -2.0, 'novel': 1.3, 'good': 1.9, 'surprises': 0.9, 'worth': 0.9, 'bad': -2.5, 'highlight': 1.4, 'loses': -1.3, 'chance': 1.0, 'pretty': 2.2, 'haunted': -2.1, 'haunting': -1.1, 'disappointingly': -1.9, 'original': 1.3, 'evils': -2.7, 'inspired': 2.2}, 'neg'), ({'playing': 0.8, 'miss': -0.6, 'villain': -2.6, 'avoid': -1.2, 'beautiful': 2.9, 'confused': -1.3, 'wishing': 0.9, 'awful': -2.0, 'confrontation': -1.3, 'sharing': 1.8, 'fails': -1.8, 'enchanted': 1.6, 'humor': 1.1, 'successfully': 2.2, 'conspiracy': -2.4, 'trite': -0.8, 'disappointing': -2.2, 'successful': 2.8, 'forget': -0.9, 'cute': 2.0, 'funny': 1.9, 'menace': -2.2, 'award': 2.5, 'alone': -1.0, 'supporting': 1.9, 'better': 1.9, 'mature': 1.8, 'recommend': 1.5, 'wrong': -2.1, 'bad': -2.5, 'peaceful': 2.2, 'pretty': 2.2, 'ability': 1.3, 'attractive': 1.9, 'negative': -2.7, 'tranquility': 1.8, 'steals': -2.3, 'admiral': 1.3, 'terrific': 2.1, 'best': 3.2, 'humorous': 1.6, 'tranquil': 0.2, 'fail': -2.5, 'agitated': -2.0, 'wars': -2.6, 'violence': -3.1, 'optimistic': 1.3, 'significance': 1.1, 'love': 3.2, 'critics': -1.2, 'no': -1.2, 'restoring': 1.2, 'devoted': 1.7, 'good': 1.9, 'evil': -3.4, 'enjoyed': 2.3, 'winner': 2.8, 'lowbrow': -1.9, 'fondness': 2.5, 'original': 1.3, 'okay': 0.9, 'like': 1.5, 'adequate': 0.9, 'battles': -1.6, 'surprisingly': 1.2, 'shares': 1.2, 'fan': 1.3, 'undermined': -1.5, 'wish': 1.7, 'plays': 1.0, 'special': 1.7, 'broken': -2.1, 'relief': 2.1, 'exploration': 0.9, 'well': 1.1, 'rejects': -2.2, 'healthy': 1.7, 'fine': 0.8, 'promise': 1.3}, 'pos')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (c)\n",
        "\n",
        "Since we now have two different datasets to create two classifiers for, we will want to reduce the amount of redundant code we will write by writing functions. For this part, you will write a function for splitting the lists we made into training and test sets.\n",
        "\n",
        "Define the function `split_train_and_test` such that:\n",
        "* the first parameter is the dataset, presumed to be a list \n",
        "* the second parameter is the percentage (presumed to be a value between 0.0 and 1.0) to be used for the training set; the default value for this parameter should be 0.9\n",
        "\n",
        "*If you are not familar with how to set default values for parameters in Python, see [this page](https://www.w3schools.com/python/gloss_python_function_default_parameter.asp).*\n",
        "  \n",
        "The function should then get separate lists for the train (the first portion of the list) and test (the last portion of the list) sets, then return both lists as a tuple, i.e. `(train_set, test_set)`.\n",
        "\n",
        "Just define the function here. We will call it in the next part."
      ],
      "metadata": {
        "id": "7vXROWpcHnEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "def split_train_and_test(dataset, train_percent=0.9):\n",
        "\n",
        "    # Determine the size of the training set based on the percentage\n",
        "    train_size = int(len(dataset) * train_percent)\n",
        "    \n",
        "    # Split the dataset into training and testing sets\n",
        "    train_set = dataset[:train_size]\n",
        "    test_set = dataset[train_size:]\n",
        "    \n",
        "    # Return the training and testing sets as a tuple\n",
        "    return (train_set, test_set)\n"
      ],
      "metadata": {
        "id": "E0HWRntGxoYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (d)\n",
        "\n",
        "Call your `split_train_and_test` function to split your two formatted lists into train and test. You should end up with \n",
        "* a training set and test set for the documents with *word counts* as the feature set, and \n",
        "* a training set and test set for the documents with *sentiment scores* as the feature set.\n",
        "\n",
        "**Train two separate Naive Bayes Classifiers**, one for each training set. Remember that `NaiveBayesClassifier.train(train_set)` is the function call for creating a classifier this way.\n",
        "\n",
        "Print the 10 most informative features features for each classifier to confirm the models were trained."
      ],
      "metadata": {
        "id": "LANYvrYBJI8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "# Import necessary modules\n",
        "from nltk.classify import accuracy\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import apply_features\n",
        "\n",
        "# Split the word_counts list into train and test sets\n",
        "train_set_wc, test_set_wc = split_train_and_test(word_counts)\n",
        "\n",
        "# Train a NaiveBayesClassifier on the word_counts train_set\n",
        "classifier_wc = NaiveBayesClassifier.train(train_set_wc)\n",
        "\n",
        "# Print the 10 most informative features for the word_counts classifier\n",
        "print(\"Word Count Classifier\")\n",
        "print(classifier_wc.show_most_informative_features(10))\n",
        "\n",
        "# Split the sentiment_docs list into train and test sets\n",
        "train_set_sen, test_set_sen = split_train_and_test(sentiment_docs)\n",
        "\n",
        "# Train a NaiveBayesClassifier on the sentiment_docs train_set\n",
        "classifier_sen = NaiveBayesClassifier.train(train_set_sen)\n",
        "\n",
        "# Print the 10 most informative features for the sentiment classifier\n",
        "print(\"Sentiment Classifier\")\n",
        "print(classifier_sen.show_most_informative_features(10))\n"
      ],
      "metadata": {
        "id": "TB5pMdwVzMKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0073e8e3-8c05-400a-c376-cfe1bd8acde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count Classifier\n",
            "Most Informative Features\n",
            "               ludicrous = 1                 neg : pos    =     14.9 : 1.0\n",
            "            breathtaking = 1                 pos : neg    =     13.5 : 1.0\n",
            "               wonderful = 2                 pos : neg    =     13.4 : 1.0\n",
            "                  avoids = 1                 pos : neg    =     12.7 : 1.0\n",
            "                  boring = 2                 neg : pos    =     11.6 : 1.0\n",
            "              astounding = 1                 pos : neg    =     11.4 : 1.0\n",
            "                     bad = 5                 neg : pos    =     11.2 : 1.0\n",
            "               affecting = 1                 pos : neg    =     10.8 : 1.0\n",
            "              apparently = 2                 neg : pos    =     10.5 : 1.0\n",
            "                  stupid = 2                 neg : pos    =     10.4 : 1.0\n",
            "None\n",
            "Sentiment Classifier\n",
            "Most Informative Features\n",
            "               ludicrous = -1.5              neg : pos    =     14.9 : 1.0\n",
            "                  avoids = -0.7              pos : neg    =     12.7 : 1.0\n",
            "              astounding = 1.8               pos : neg    =     11.4 : 1.0\n",
            "             outstanding = 3.0               pos : neg    =     10.4 : 1.0\n",
            "               insulting = -2.2              neg : pos    =     10.0 : 1.0\n",
            "            breathtaking = 2.0               pos : neg    =      9.9 : 1.0\n",
            "                   sucks = -1.5              neg : pos    =      9.6 : 1.0\n",
            "                     dud = -1.0              neg : pos    =      9.2 : 1.0\n",
            "                  hatred = -3.2              pos : neg    =      8.8 : 1.0\n",
            "                   jewel = 1.5               neg : pos    =      8.5 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Evaluation\n",
        "\n",
        "*This question references the variables from question 1, so be sure to complete all previous parts before starting this one.*\n",
        "\n",
        "Evaluation can be done using many methods and many metrics, as discussed in lecture. For this question, we will ultimately be calculating F1 scores for each model."
      ],
      "metadata": {
        "id": "5btc8R_LKCTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (a)\n",
        "\n",
        "Define a function called `confusion_matrix` such that\n",
        "* the first parameter is a classifier with the function `classify` (which `NaiveBayesClassifier` does)\n",
        "* the second parameter is a test set with documents formatted as we did in question 1\n",
        "\n",
        "`confusion_matrix` should calculate the number of True Positives, False Positives, True Negatives, and False Negatives and return them in that order as a tuple. *(Remember that despite the name sounding like \"two-ple\", tuples can contain any number of elements.)* Similar to how you calculated accuracy in the previous assignment, calculating TP, FP, TN, and FN can be done by\n",
        "* iterating over the test set, classifying each document on its features\n",
        "* comparing the predicted label with the document's actual label\n",
        "\n",
        "This part only defines this function. It will be called in the next part."
      ],
      "metadata": {
        "id": "eiQXRPEYK73w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "def confusion_matrix(classifier, test_set):\n",
        "    tp = fp = tn = fn = 0\n",
        "    for features, label in test_set:\n",
        "        predicted_label = classifier.classify(features)\n",
        "        if label == 'pos':\n",
        "            if predicted_label == 'pos':\n",
        "                tp += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "        else:\n",
        "            if predicted_label == 'pos':\n",
        "                fp += 1\n",
        "            else:\n",
        "                tn += 1\n",
        "    print(tp,fp,tn,fn)\n",
        "    return tp, fp, tn, fn\n"
      ],
      "metadata": {
        "id": "0uNYLDwhoEy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "\n",
        "Define a function called `f1_score` that, much like `confusion_matrix`, takes a classifier and a test set as parameters. The function should\n",
        "* call `confusion_matrix` with the given parameters in order to calculate TP, FP, TN, and FN, then\n",
        "* calculate and return the F1-score using the following formulas\n",
        "$$\n",
        "Precision = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Recall = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "$$\n",
        "F_1 = \\frac{2PR}{P+R}\n",
        "$$\n",
        "\n",
        "Call `f1_score` on each classifier and its corresponding test set and print the results."
      ],
      "metadata": {
        "id": "RxSFIk_RMF1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "def f1_score(classifier, test_set):\n",
        "    tp, fp, tn, fn = confusion_matrix(classifier, test_set)\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    return f1\n",
        "\n",
        "\n",
        "word_count_f1 = f1_score(classifier_wc, test_set_wc)\n",
        "sentiment_f1 = f1_score(classifier_sen, test_set_sen)\n",
        "\n",
        "print(\"F1-score for word count classifier:\", word_count_f1)\n",
        "print(\"F1-score for sentiment classifier:\", sentiment_f1)\n"
      ],
      "metadata": {
        "id": "HEZjtDO3MC08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2560122f-7585-4439-cc8e-786815dfc899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87 47 62 4\n",
            "82 45 64 9\n",
            "F1-score for word count classifier: 0.7733333333333333\n",
            "F1-score for sentiment classifier: 0.7522935779816514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before answering the last part of this question, you can (optionally) perform a sort of \"soft\" cross-validation with your code by re-running all of your code cells to see how much the final F1 scores change each time. *(In Colab, go to Runtime -> Run all.)* Each run will probably take about 5-10 seconds depending on your connection. You should see that the results do not change too much each time."
      ],
      "metadata": {
        "id": "jGGnm72uUp4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (c)\n",
        "\n",
        "What do you observe about the resulting F1 scores? Are they similar? If so, why do you think the performance does not differ much between the two models. If one score is notably higher than other, why might the higher-scoring model perform better?\n",
        "\n",
        "**Provide your thoughts in the text cell below.** Your answer should have at least a couple sentences with insight regarding the differences between the features used in each classifier."
      ],
      "metadata": {
        "id": "jJgrFFpIMCRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting F1 scores for the word count and sentiment classifiers are similar, with a difference of only about 0.02, and a total accuracy ranging from 75%-82% across runs. This suggests that both classifiers are performing reasonably well and that the choice of feature set here does not make a significant difference in performance.\n",
        "\n",
        "It's worth noting that the informative features for each classifier are quite different, with the word count classifier placing more weight on specific words like \"ludicrous\" and \"breathtaking,\" while the sentiment classifier seems to place more emphasis on words like \"hatred\" and \"sucks\" but also considers words like \"ludicrous\" and \"avoids\", similar to the word count classifier.\n",
        "\n",
        "These differences show that while both classifiers are using different datasets, both feature sets seem to be effective in distinguishing between positive and negative reviews with reasonable accuracy."
      ],
      "metadata": {
        "id": "5GVaE8GSOT7a"
      }
    }
  ]
}